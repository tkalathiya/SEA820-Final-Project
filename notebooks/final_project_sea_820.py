# -*- coding: utf-8 -*-
"""Final Project -  SEA 820.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11ksndz0P_CZ0498AjJ1IwzII1SQB0ibR

#Foundations and Classic Model

## 1. Team Formation & Setup

## 2.   Data Exploration and Preprocessing

### a. Load the Dataset
"""

# mount Google Drive to access files
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Load dataset
df = pd.read_csv("/content/drive/My Drive/Colab Notebooks/AI_Human.csv")
df.head()

# check dataset structures
df.info()
df.describe()

"""### b. Exploratory Data Analysis

"""

# analyze text length

import matplotlib.pyplot as plt

df['text_length'] = df['text'].apply(lambda x: len(str(x).split()))

# Plot histogram
plt.figure(figsize=(10, 5))
plt.hist(df['text_length'], bins=50, color='skyblue', edgecolor='black')
plt.title('Distribution of Text Lengths')
plt.xlabel('Number of Words')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""
**X-axis = Number of words per text entry.**

**Y-axis = Frequency (how many texts have that length).**

**Shape:**

The distribution is right-skewed, meaning:

* Most texts have a word count between 200 to 500 words.

* Some texts go over 1000 words, but they are rare (long tail).

**Why This Is Important?**

1. Feature Engineering Insight:

* Helps identify if very short or long texts need special handling.

* For example, LLMs may generate more structured responses.

2. Modeling Decisions:

* Could inform padding strategy for models like BERT.

* You may later filter out extreme outliers (e.g., texts >1500 words).

3. Bias Detection:

* If AI-generated texts tend to be longer/shorter, this can cause model bias.

* This chart helps you discover and correct that."""

# analyze class distribution

import seaborn as sns

plt.figure(figsize=(6,4))
sns.countplot(x='generated', data=df)
plt.title("Class Distribution: Human (0) vs AI (1)")
plt.xlabel("Label")
plt.ylabel("Count")
plt.show()

"""**X-axis: Represents the class labels (0 = Human, 1 = AI).**

**Y-axis: Shows the count (number of examples) for each label.**


We can interpret that:

* There are more human-written texts (approximately 310,000) than AI-generated ones (approximately 180,000).

* The dataset is somewhat imbalanced, but not severely.


The dataset showed moderate class imbalance, with significantly more human-written texts than AI-generated ones. This imbalance could lead to bias in classification if not addressed properly. We used stratified sampling during train/test splitting to preserve this ratio across sets.




"""

from sklearn.model_selection import train_test_split

X = df['text']
y = df['generated']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training samples: {len(X_train)}, Testing samples: {len(X_test)}")

"""### c. Data Preprocessing and Tokenization Pipeline"""

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))

def clean_text(text):
    text = text.lower()                             # lowercase
    text = re.sub(r'[^\w\s]', '', text)             # remove punctuation
    text = re.sub(r'\d+', '', text)                 # remove numbers
    tokens = text.split()                           # tokenize
    tokens = [word for word in tokens if word not in stop_words]
    return " ".join(tokens)

df['clean_text'] = df['text'].apply(clean_text)

"""## 3. Implement Baseline Model

### a. TF_IDF and CLassic ML classifier
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# TF-IDF Vectorizer
vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

"""### b. Logistic Regression Model"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# Train classifier
clf = LogisticRegression(max_iter=1000)
clf.fit(X_train_tfidf, y_train)

# Predict
y_pred = clf.predict(X_test_tfidf)

"""**The reason behind going with Logistic Regression as a classic baseline model is that it is specifically designed to handle binary classification tasks, making it a solid baseline model.**

We are solving a binary classification problem:

Label 0: Human-written text

Label 1: AI-generated text

### c. Evaluate Performance
"""

# Classification report
print(classification_report(y_test, y_pred, target_names=["Human", "AI"]))

# Confusion matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Human", "AI"], yticklabels=["Human", "AI"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Logistic Regression")
plt.show()

"""**The logistic regression model, using TF-IDF features, achieved a 99% accuracy. Both human and AI classes were predicted with high precision and recall, confirming that classical models can perform extremely well when combined with well-engineered features. Misclassifications were least and mostly related to overlaps between AI and human text.**

What we found?

* True Positives (AI correctly identified): 35,908

* True Negatives (Human correctly identified): 60,996

* False Positives (Human predicted as AI): 163

* False Negatives (AI predicted as Human): 380

What does it mean?

* Model is highly reliable on this dataset.

* TF-IDF + Logistic Regression is already giving near-perfect results.

* Moving to BERT may only give marginal gains, but would better handle edge cases or semantic complexity.

# Fine-Tuning a Transformer Model

## 1. Hugging Face Datasets and Transformers

### a. Load Data using Hugging Face Datasets Library
"""

!pip install datasets transformers

from datasets import Dataset
import pandas as pd

# train/test dataframe
train_df = pd.DataFrame({'text': X_train, 'label': y_train})
test_df = pd.DataFrame({'text': X_test, 'label': y_test})

# Convert to Hugging Face Dataset
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

"""### b. Load Pre-Trained Tokenizer"""

from transformers import DistilBertTokenizerFast

# Load DistilBERT tokenizer
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

"""### c. Tokenize the dataset"""

# Tokenization function
def tokenize_function(batch):
    tokenized_batch = tokenizer(batch["text"], padding="max_length", truncation=True, max_length=256)
    tokenized_batch["label"] = batch["label"].unsqueeze(1)
    return tokenized_batch

# correct labels
train_dataset = train_dataset.map(lambda x: {"label": int(x["label"])})
test_dataset = test_dataset.map(lambda x: {"label": int(x["label"])})

"""### d. Format for PyTorch Training"""

# Set the format for PyTorch
train_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])
test_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])

"""## 2. Fine-Tuning

### a. Load Pre-Trained Model
"""

from transformers import DistilBertForSequenceClassification

# Binary classification
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)

"""### b. Define Training Arguments"""

from transformers import TrainingArguments

# training args
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    save_total_limit=1,
    report_to="none"
)

"""### c. Define Evaluation Metric"""

from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import numpy as np

def compute_metrics(pred):
    logits, labels = pred
    preds = np.argmax(logits, axis=1)
    # Use 'binary' average for binary classification
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

"""### d. Init Trainer and start Fine-Tuning"""

# using subset i.e 5% of the data
small_train = train_dataset.shuffle(seed=42).select(range(int(0.05 * len(train_dataset))))
small_test = test_dataset.shuffle(seed=42).select(range(int(0.05 * len(test_dataset))))

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train,
    eval_dataset=small_test,
    compute_metrics=compute_metrics,
)
trainer.train()

"""**Observations:**

* Very high accuracy (~99%), the model learned the classification task very well, even with only 5% of the data.

* F1-score improves across epochs, peaking in epoch 3.

* Precision and Recall both improve, with recall being consistently strong (meaning the model is good at identifying AI text).

* Validation loss drops overall, showing no signs of overfitting, even with small data.

### e. Evaluate the model
"""

trainer.evaluate()

"""## 3. Initial Evaluation and Comparison

### a. Evaluate Fine-Tuned DistilBERT Model
"""

results = trainer.evaluate()
print(results)

"""### b. Compare against Baseline Model

| Metric    | **Logistic Regression** | **DistilBERT (Transformer)** | **Difference** |
| --------- | ------------------- | ---------------------------- | -------------- |
| Accuracy  | 0.99                | 0.99            | ±0.00      |
| Precision | 0.99–1.00           | 0.98           | ±0.01     |
| Recall    | 0.99–1.00           | 0.99            | ±0.01       |
| F1 Score  | 0.99                | 0.98            | ±0.01       |

Explanation:

1. No, the DistilBERT model did not outperform the baseline.

2. The Logistic Regression model had slightly higher accuracy and F1-score.

3. The difference is very small (0.5%–1%), but the baseline was already extremely strong.

4. This suggests that:


  *   Classic ML with TF-IDF can still be     powerful in binary classification tasks with clear patterns.
  *   Transformer models need longer training, more data, or better tuning to outperform in such cases.
  *   Transformer adds computational cost, so if your goal is only performance, the baseline is better.
"""